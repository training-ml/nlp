{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gensim?\n",
    "- `Gensim = “Generate Similar”` is a popular open source natural language processing (NLP) library used for unsupervised topic modeling.\n",
    "\n",
    "\n",
    "- Let's understand important terms and its meaning.\n",
    "\n",
    "\n",
    " Document: some text.\n",
    "\n",
    " Corpus: a collection of documents.\n",
    "\n",
    " Vector: a mathematically convenient representation of a document.\n",
    "\n",
    " Model: an algorithm for transforming vectors from one representation to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Document: some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corpus: a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector: a mathematically convenient representation of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "# Creating dictionaries, which helps during Topic modelling\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "# Vectore representation \n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: an algorithm for transforming vectors from one representation to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Always make a practice of testing small texts when trying something new\n",
    "# in each tuple below 1st occurance is ID and 2nd occurance is count\n",
    "\n",
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'details': 0, 'files': 1, 'sample': 2, 'text': 3, 'how': 4, 'open': 5, 'to': 6, 'txt': 7, 'basic': 8, 'informations': 9, 'create': 10, 'download': 11, 'file': 12, 'of': 13, 'url': 14, 'mb': 15}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess,\n",
    "\n",
    "import os\n",
    "\n",
    "dict_STF = corpora.Dictionary(simple_preprocess(line, deacc =True) for line in open(r\"sample.txt\"))\n",
    "\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Latent Dirichlet allocation )\n",
    "\n",
    "It is one of the important module when it comes to Topic Modelling. We will work on this later, but lets understand what it does.\n",
    "\n",
    "\n",
    "Automatically extracting information about topics from large volume of texts in one of the primary applications of NLP (natural language processing). Large volume of texts could be feeds from hotel reviews, tweets, Facebook posts, feeds from any other social media channel, movie reviews, news stories, user feedbacks, e-mails etc.\n",
    "\n",
    "In this digital era, to know what people/customers are talking about, to understand their opinions, and their problems, can be highly valuable for businesses, political campaigns and administrators. But, is it possible to manually read through such large volumes of text and then extracting the information from topics?\n",
    "\n",
    "No, it’s not. It requires an automatic algorithm that can read through these large volume of text documents and automatically extract the required information/topics discussed from it.\n",
    "\n",
    "Role of LDA\n",
    "\n",
    "LDA’s approach to topic modeling is to classify text in a document to a particular topic. Modeled as Dirichlet distributions, LDA builds −\n",
    "\n",
    "- A topic per document model and\n",
    "- Words per topic model\n",
    "\n",
    "After providing the LDA topic model algorithm, in order to obtain a good composition of topic-keyword distribution, it re-arrange −\n",
    "\n",
    "- The topics distributions within the document and\n",
    "- Keywords distribution within the topics\n",
    "\n",
    "\n",
    "While processing, some of the assumptions made by LDA are −\n",
    "\n",
    "- Every document is modeled as multi-nominal distributions of topics.\n",
    "- Every topic is modeled as multi-nominal distributions of words.\n",
    "- We should have to choose the right corpus of data because LDA assumes that each chunk of text contains the related words.\n",
    "- LDA also assumes that the documents are produced from a mixture of topics.\n",
    "\n",
    "\n",
    "Note-  **Will discuss more about this during topic modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
